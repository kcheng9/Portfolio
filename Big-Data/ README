Big Data projects build on each other ending in a final project.
Each of these begins with a data set that is transformed using Big Data tools.
Homeworks predicted mortality in patients.
Code is found under code folder. Example data is provided, but not all data will be available due to sensitivity.

Homework 1:
Given raw data in CSV files of event sequences containing
patient number, diagnosis code, diagnosis name, date, and diagnosis measurement,
use these to create and compare predictive models for whether a patient is alive or dead.
The raw data is filtered into usable data, then split into observation window (time that a patient's events are observed)
and prediction window (time when the prediction must be made of alive or dead per patient).
Then values are filtered and calculated into aggregate values per patient.
They are then reformatted into a format usable by available modeling libraries (logistic regression, SVM, and Decision Tree).
Each model was then cross-validated to determine performance of the model.

Homework 2:
Use Big Data tools to find descriptive statistics and transform the data to be analyzed.
Use HIVE to compute various metrics in the data.
Use PIG to transform the data into usable data
(similar to in Homework 1 where it is separated into different windows and filtered / calculated and formatted into the correct format).
Use Hadoop to train multiple logistic regression classifiers (similar to Homework 1 but adding Hadoop for parallel processing).
Compare classifiers that did not use Hadoop and did use Hadoop.

Homework 3: 
